model: gpt-4
time: 0:02:11.592218
question: from __future__ import annotations
import json
import os
import logging
import time

try:
    # from spiralchain.chains.nttpc_chat import NTTPCCharacterChain

    # from langchain.chains.conversational_retrieval.prompts import (
    #    CONDENSE_QUESTION_PROMPT,
    #    QA_PROMPT,
    # )

    from decorators import handle_exception, notify_handler_call

    # from chain import get_websocket_callback_manager
    from params import Params
    from models import HistoryMeta, ResourceType, ChatHistory, AnnotationMeta
    from repository import PromptSettingClient
    from repository.vector_db import VectorDBClient
    from repository.repository import (
        ResourceMetaRepository,
        ChatHistoryRepo,
        ResourceMetaClient,
        ChatHistoryClient,
    )

except Exception:
    import sys

    sys.path.append("amplify/backend/function/kittbackenddevUtilLayer/lib/python")
    # from spiralchain.chains.nttpc_chat import NTTPCCharacterChain

    # from langchain.chains.conversational_retrieval.prompts import (
    #    CONDENSE_QUESTION_PROMPT,
    #    QA_PROMPT,
    # )

    from decorators import handle_exception, notify_handler_call

    # from chain import get_websocket_callback_manager
    from params import Params
    from models import HistoryMeta, ResourceType, ChatHistory, AnnotationMeta
    from repository import PromptSettingClient
    from repository.vector_db import VectorDBClient
    from repository.repository import (
        ResourceMetaRepository,
        ChatHistoryRepo,
        ResourceMetaClient,
        ChatHistoryClient,
    )

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

env = os.environ["ENV"]

from langchain.chains import LLMChain
from langchain.prompts.prompt import PromptTemplate
from langchain.chat_models import ChatOpenAI, AzureChatOpenAI

from typing import Any, Dict, List, Union

import boto3
from langchain.schema import AgentAction, AgentFinish, LLMResult

from langchain.callbacks.base import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
# import tiktoken


class WebsocketCallbackHandler(BaseCallbackHandler):
    """callbak handler for aws websocket"""

    def __init__(
        self,
        connection_id: str,
        domain_name: str,
        stage: str,
        referred_docs: list,
        splitter=None,
    ) -> None:
        super().__init__()
        self.connection_id = connection_id
        self.domain_name = domain_name
        self.stage = stage
        self.referred_docs = referred_docs
        self.client = boto3.client(
            "apigatewaymanagementapi", endpoint_url=f"https://{domain_name}/{stage}"
        )
        self.splitter = splitter

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        if self.splitter:
            token = self.splitter.apply_stream(token)
            if token == "":
                return
        data = json.dumps({"type": "msg", "message": token})
        self.client.post_to_connection(Data=data, ConnectionId=self.connection_id)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        data = json.dumps({
            "type": "end",
            "documents": self.referred_docs
        })
        self.client.post_to_connection(Data=self.end_message, ConnectionId=self.connection_id)
        self.client.close()
        self.client.delete_connection(ConnectionId=self.connection_id)

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when LLM errors."""
        self.client.close()
        self.client.delete_connection(ConnectionId=self.connection_id)
        raise error

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Run when tool starts running."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""
        pass

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> None:
        """Run on arbitrary text."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent finish."""


def get_websocket_callback_manager(
    connection_id: str,
    domain_name: str,
    stage: str,
    referred_docs: list,
    splitter=None,
):
    """Callback manager with its own handler for AWS Websocket API."""
    cbHandler = WebsocketCallbackHandler(
        connection_id=connection_id,
        domain_name=domain_name,
        stage=stage,
        referred_docs=referred_docs,
        splitter=splitter,
    )
    return CallbackManager([cbHandler])


# api_key = os.environ["OPENAI_API_KEY"]
# pinecone_api_key = os.environ["PINECONE_API_KEY"]

api_key = Params().get_parameter(os.environ["OPENAI_API_KEY"])
pinecone_api_key = Params().get_parameter(os.environ["PINECONE_API_KEY"])


# prep repository
resource_meta_repository = ResourceMetaRepository(
    resource_client=ResourceMetaClient(env=env),
    vector_client=VectorDBClient(
        api_key=pinecone_api_key,
        env=env,
        openai_api_key=api_key,
    ),
)
prompts_client = PromptSettingClient(env)
history_meta_repository = ResourceMetaRepository(ResourceMetaClient(env))
chat_history_repository = ChatHistoryRepo(ChatHistoryClient(env))

# created_at, bot_id ãŒãƒ•ãƒ­ãƒ³ãƒˆã§æœªå®Ÿè£…

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®šç¾©
_template = """ä»¥ä¸‹ã® Chat History ã‚’è¸ã¾ãˆã¦ã€Follow Up Input ã‚’ Standalone question ã«è¨€ã„ç›´ã—ã¦ãã ã•ã„ã€‚

====
Chat History:
{chat_history}
====
Follow Up Input: {question}
Standalone question:"""
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)


@handle_exception(logger)
@notify_handler_call(logger)
def handler(event, context):
    body = (
        json.loads(event["body"]) if isinstance(event["body"], str) else event["body"]
    )
    bot_id = (
        body["bot_id"] if "bot_id" in body else "9f219cf1-d666-407b-bf27-a3cbd0d73460"
    )
    return post_handler(event, bot_id, body)


# ã“ã‚Œã¯ãªãœé–¢æ•°åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã‹
def put_history_meta(history_meta: HistoryMeta):
    return history_meta_repository.put(history_meta)


# ã“ã‚Œã¯ãªãœé–¢æ•°åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã‹
def put_chat_history(chat_history: ChatHistory):
    return chat_history_repository.put(chat_history)


class Splitter:
    """
    Streamã•ã‚Œã¦ãã‚‹æ–‡å­—åˆ—ã®ä¸­ã§ã€ç‰¹å®šã®æ–‡å­—åˆ—ã‚’è¦‹ã¤ã‘ã¦ã€å‡ºåŠ›ã®åˆ¶å¾¡ã‚’è¡Œã†ã€‚
    GPTã®å‡ºåŠ›ãŒä¸å®‰å®šã§ã€ç‰¹å®šã®æ–‡å­—åˆ—ãŒå‡ºåŠ›ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ãã®å ´åˆã¯safe_close()ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§å…¨æ–‡ã‚’å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
    """

    def __init__(self, split_conditions):
        """
        Args:
            split_conditions (list): å‡ºåŠ›é–‹å§‹ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã€‚ã©ã‚Œã‹ä¸€ã¤ã§ã‚‚å‡ºç¾ã—ãŸã‚‰ã€å‡ºåŠ›ã‚’é–‹å§‹ã™ã‚‹ã€‚
            ç©ºã®å ´åˆã¯ã€Splitterè‡ªä½“ãŒç„¡åŠ¹åŒ–ã•ã‚Œã€ã™ã¹ã¦ã®æ–‡å­—åˆ—ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã€‚
        """
        self.conditions = split_conditions
        self.text = ""
        self.output = ""
        if split_conditions:
            self.flag = False  # ä½•ã‚‰ã‹ã®æ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã¨ãã«ã¯ã€æ–‡å­—ã‚’æ¨ã¦ã‚‹
        else:
            self.flag = True  # æ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã¨ãã¯ã€å¸¸ã«å‡ºåŠ›

    def safe_close(self):
        """
        ã‚‚ã—ä½•ã‚‚å‡ºåŠ›ã•ã‚Œã¦ã„ãªã‹ã£ãŸå ´åˆã€å…¥åŠ›ã•ã‚ŒãŸå…¨æ–‡ã‚’è¿”ã™ã€‚
        å‡ºåŠ›ã•ã‚Œã¦ã„ãŸå ´åˆã¯ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã™ã®ã§ã€apply_streamã®å¾Œã«ã“ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã›ã°ã€å¸¸ã«æ­£ã—ã„æ–‡ç« ã‚’å–å¾—ã§ãã‚‹ã€‚
        """
        if self.output == "":
            return self.text
        else:
            return ""

    def apply(self, text):
        """
        Streamã§ã¯ãªãã¦ã€ä¸€æ°—ã«æ–‡å­—åˆ—ã‚’æ¸¡ã™å ´åˆã«ä½¿ã†ã€‚
        Args:
            text (str): å…¥åŠ›æ–‡å­—åˆ—
        Returns:
            str: å‡ºåŠ›æ–‡å­—åˆ—
        """
        result = ""
        for c in text:
            result += self.apply_stream(c)
        return result + self.safe_close()

    def apply_stream(self, token):
        """
        Streamã§æ–‡å­—ã‚’æ¸¡ã™å ´åˆã«ä½¿ã†ã€‚
        Args:
            token (str): å…¥åŠ›æ–‡å­—
        Returns:
            str: å‡ºåŠ›æ–‡å­—ã€‚conditionãŒæº€ãŸã•ã‚Œã¦ã„ãªã„é–“ã¯ã€ç©ºæ–‡å­—åˆ—ãŒè¿”ã•ã‚Œã‚‹ã€‚
        """

        # do_output = copy.deepcopy(self.flag)
        discard_this = False

        # ãƒ•ãƒ©ã‚°ã®åˆ¶å¾¡
        for tgt in self.conditions:
            f1 = tgt in self.text
            f2 = tgt in (self.text + token)
            if (not f1) and f2:
                self.flag = True
                discard_this = True  # ã“ã®æ–‡å­—ã¾ã§ã¯å‡ºåŠ›ã—ãªã„
        self.text += token

        # å‡ºåŠ›ã®åˆ¶å¾¡
        if self.flag:
            if discard_this:
                return ""
            elif token in [" "] and self.output == "":
                return ""
            else:
                self.output += token
                return token
        else:
            return ""


def post_handler(event, bot_id: str, request_body: dict):
    try:
        company_id = request_body["company_id"]
        domain_name = event.get("requestContext", {}).get("domainName")
        stage = event.get("requestContext", {}).get("stage")
        connectionId = event.get("requestContext", {}).get("connectionId")
        user_id = request_body["user_id"]
        user_input = request_body["user_input"]

        logger.info(f"request body: {request_body}")
        logger.info(f"domain_name: {domain_name}")
        logger.info(f"stage: {stage}")
        logger.info(f"connectionId: {connectionId}")
        user_posted_at = int(time.time())

        # get bot
        bot = resource_meta_repository.get(
            resource_type=ResourceType.bot,
            company_id=company_id,
            resource_id=bot_id,
        )
        # TODO check existency?
        logger.info(f"bot: {bot.to_dict()}")

        prompt_config_id = "example.json"  # bot.prompt
        prompt_config = json.loads(prompts_client.get_default(prompt_config_id))
        logger.info(f"prompt_config: {prompt_config}")

        qa_prompt_with_character = load_config(
            prompt_config,
            key=["PromptSettings", "UserPrompt", "Prompt"],
            default=None,
            format=True,
        )
        qa_prompt_with_character = PromptTemplate(
            template=qa_prompt_with_character,
            input_variables=["context", "question"],
        )

        # LLMã®ä¿®æ­£å±¥æ­´ã‚’å–å¾— (ç¾åœ¨ã¯ã¾ã ä½¿ã‚ãªã„ã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã‚‹)
        # annotations_pairs = resource_meta_repository.search_annotation_pairs(company_id, user_input, annos_refs)
        # annotations = build_annotation_string(annotations_pairs)
        # logger.info(annotations)


        # LLMãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
        llm_model = "openai"  # Azureã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸå ´åˆã¯ã€ã“ã“ã‚’å‹•çš„ã«å¤‰æ›´
        model_name = "gpt-3.5-turbo"  # å°†æ¥ã€ã“ã“ã‚‚å‹•çš„ã«å¤‰æ›´

        # Define tokenizer
        # tokenizer = tiktoken.encoding_for_model(model_name)
        if model_name == "gpt-3.5-turbo":
            max_tokens = 4096 - 512  # 512ã¯å›ç­”ã®ãŸã‚ã®ä½™ç™½
        elif model_name == "gpt-4":
            max_tokens = 8192 - 512  # 512ã¯å›ç­”ã®ãŸã‚ã®ä½™ç™½
        else:
            raise NotImplementedError(f"Invalid model_name: {model_name}")

        # ã“ã“ã‹ã‚‰ã€LangChainã‚’å‘¼ã³å‡ºã—

        ###############################################
        ## Step1: éå»ã®è³ªå•å±¥æ­´ã‚’çµ±åˆã—ã¦ã€å˜ä¸€ã®è³ªå•ã«ã™ã‚‹
        ### Greeting PromptãŒã‚ã‚‹å ´åˆã¯ç„¡è¦–ã€‚å°‘ãªãã¨ã‚‚ä¸€å¯¾ã®QAãŒã‚ã‚‹å ´åˆã«condenseã™ã‚‹ã€‚

        chat_history = request_body["history"]
        user_question = request_body["user_input"]
        max_history_length = None
        num_tokens = None
        trial_prompt = None

        # è³ªå•ç”Ÿæˆ chain ã‚’ä½œæˆ
        llm_without_streaming = build_llm_model(
            api_key=api_key,
            model_name=model_name,
            need_streaming=False,
            verbose=True,
            llm_model=llm_model,
        )

        if len(chat_history) >= 2:
            chain_condense_question = LLMChain(
                llm=llm_without_streaming,
                prompt=CONDENSE_QUESTION_PROMPT,
                verbose=True,
            )
            # è³ªå•å±¥æ­´ã‚‚å«ã‚ã¦ã€åˆè¨ˆtokenæ•°ãŒmax_tokensã‚’è¶…ãˆãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚ã¾ãŸã€ã„ãšã‚Œã«ã›ã‚ˆ6å›ä»¥ä¸Šã®è³ªå•å±¥æ­´ã¯ç„¡è¦–ã™ã‚‹ã€‚
            # ãŸã£ãŸ1ä»¶ã§ã‚‚contexté•·ã‚’è¶…ãˆã¦ã—ã¾ã†å ´åˆã¯ã€ã€å…ƒã®è³ªå•ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
            for history_length in range(min(6, len(chat_history)), 0, -2):
                trial_prompt = CONDENSE_QUESTION_PROMPT.format(
                    chat_history=convert_chat_history(chat_history[-history_length:]),
                    question=user_question,
                )
                num_tokens = 0  # len(tokenizer.encode(trial_prompt))

                if num_tokens <= max_tokens:
                    max_history_length = history_length
                    break
                else:
                    continue

            if max_history_length is not None:
                condensed_question = chain_condense_question.run(
                    chat_history=convert_chat_history(
                        chat_history[-max_history_length:]
                    ),
                    question=user_question,
                    verbose=True,
                )
            else:
                condensed_question = user_question
        else:
            condensed_question = user_question

        # logger.info(convert_chat_history(chat_history))

        logger.info(
            "ğŸ‘£ chain_condense_question\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # max_history_length: {max_history_length}"
            + f" # max_tokens: {num_tokens}"
            + f" # num_tokens: {num_tokens}"
            + f" # chat_history:\n{chat_history}\n\n"
            + f" # user_question:\n{user_question}\n\n"
            + f" # condensed_question:\n{condensed_question}\n\n"
            + f" # entire prompt:\n{trial_prompt}\n\n"
        )
        # logger.info(f"user_question: {user_question}")
        # logger.info(f"condensed_question: {condensed_question}")
        # logger.info("[chain_condense_question] #End")

        ###############################################
        # Step2: VectorDBã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å–å¾—ã™ã‚‹
        ## vector-search docs for user_input
        references: list[dict[str, str]] = bot.references
        logger.info(f"references: {references}")
        docs_refs: list[str] = [
            di["resource_id"]
            for di in list(
                filter(
                    lambda item: item.get("resource_type") == "docs",
                    references,
                )
            )
        ]
        # annos_refs: list[str] = [
        #    f"annotation#{di['resource_id']}"
        #    for di in list(
        #        filter(
        #            lambda item: item.get("resource_type") == "annotation", references  # noqa
        #        )
        #    )
        # ]
        searched_docs_meta = resource_meta_repository.search_documents(
            company_id,
            condensed_question,
            docs_refs,
        )

        # æœ€å¤§tokenæ•°ä¸Šé™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«é¸å®š
        facts = ""
        num_tokens = None
        for docs_count in range(len(searched_docs_meta), -1, -1):
            trial_facts = build_facts_string(searched_docs_meta[:docs_count])
            trial_prompt = qa_prompt_with_character.format(
                context=trial_facts,
                question=condensed_question,
            )
            # num_tokens = llm_with_streaming.get_num_tokens(trial_prompt)
            num_tokens = 0  # len(tokenizer.encode(trial_prompt))
            # print(num_tokens)
            if num_tokens < max_tokens:
                facts = trial_facts
                break

        logger.info(
            "ğŸ‘£ search_docs\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # condensed_question: {condensed_question}\n"
            + f" # max_tokens: {max_tokens}\n"
            + f" # num_tokens: {num_tokens}\n"
            + f" # docs_count: {docs_count}\n\n"
            + f" # docs_refs:\n{docs_refs}\n\n"
            + f" # searched_docs_meta:\n{searched_docs_meta}\n\n"
            + f" # facts:\n{facts}\n\n"
        )

        ###############################################
        # Step3: QAã‚’å®Ÿæ–½ã™ã‚‹ã€‚

        # å‚ç…§ã—ãŸ Document ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        referred_docs = [
            {"resource_id": doc.resource_id, "resource_name": doc.resource_name, "resource_url": doc.resource_url}
            for doc in searched_docs_meta
        ]
        # websocketç”¨ã® callback handler ã‚’ç”Ÿæˆ
        callback_manager = get_websocket_callback_manager(
            connection_id=connectionId,
            domain_name=domain_name,
            stage=stage,
            referred_docs=referred_docs,
            splitter=Splitter(
                prompt_config["PromptSettings"]["UserPrompt"]["Splitter"]
            ),
        )
        # QAå›ç­”ã‚’ã™ã‚‹ chain ã‚’ä½œæˆ
        llm_with_streaming = build_llm_model(
            api_key=api_key,
            model_name=model_name,
            need_streaming=True,
            verbose=True,
            callback_manager=callback_manager,
            llm_model=llm_model,
        )

        chain_factorial_qa_with_character = LLMChain(
            llm=llm_with_streaming,
            prompt=qa_prompt_with_character,
            verbose=False,
        )
        answer_with_character = chain_factorial_qa_with_character.run(
            context=facts,
            question=condensed_question,
        )

        logger.info("[qa_prompt_with_character] #Start")
        logger.info(f"facts: {facts}")
        logger.info(f"condensed_question: {condensed_question}")
        logger.info(f"answer_with_character: {answer_with_character}")
        logger.info("[qa_prompt_with_character] #End")

        final_response = answer_with_character

        ###############################################
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜
        ## humanã‹ã‚‰ã®å•ã„ã‹ã‘ã®ä¿å­˜
        chat_history_user = ChatHistory.create(
            # user_inputã®è¨˜éŒ²,updatedæƒ…å ±ã¯ä¸è¦,chain.runã™ã‚‹å‰ã«è¨˜éŒ²
            bot_id=bot_id,
            user_id=user_id,
            created_at=request_body["created_at"]
            if "created_at" in request_body
            else 1681706665,
            posted_at=user_posted_at,
            posted_by=user_id,
            message=user_input,
        )

        logger.info(f"chat_history: {chat_history_user.to_dict()}")
        put_chat_history(chat_history=chat_history_user)

        ## aiã‹ã‚‰ã®å›ç­”ã®ä¿å­˜
        chat_history_bot = ChatHistory.create(  # botã®respã®è¨˜éŒ²,updatedæƒ…å ±ã¯ä¸è¦
            bot_id=bot_id,
            user_id=user_id,
            created_at=request_body["created_at"]
            if "created_at" in request_body
            else 1681706665,
            posted_at=int(time.time()),
            posted_by=bot_id,
            message=final_response,
        )

        logger.info(f"chat_history: {chat_history_bot.to_dict()}")
        put_chat_history(chat_history=chat_history_bot)

        logger.info("Chat history has been put.")

        ## history_metaã®ä¿å­˜
        history_meta = HistoryMeta.create(
            company_id=company_id,
            resource_id=connectionId,
            resource_name=connectionId,
            created_by=user_id,
            updated_by=user_id,
            description=final_response,
        )

        logger.info(f"history_meta: {history_meta.to_dict()}")
        put_history_meta(history_meta=history_meta)

        # ã“ã‚Œ callback ã§è¿”ã—ã¦ã„ã‚‹ã‹ã‚‰å¿…è¦ãªã„èªè­˜ã§ã™.
        response_body = json.dumps(
            {"user_input": user_input, "bot_output": final_response}
        )

    except Exception as e:
        logger.error(f"Error: {e}")
        logger.error(f"Message: {str(e)}")
        return {
            "statusCode": 500,
            "headers": {
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "OPTIONS,POST,GET",
                "Content-Type": "application/json",
            },
            "body": json.dumps({"Error": str(e)}),
        }
    else:
        return {
            "statusCode": 200,
            "headers": {
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "OPTIONS,POST,GET",
                "Content-Type": "application/json",
            },
            "body": json.dumps(response_body),
        }


def get_formatted_text(text):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã ã¨ãã«ã€ãƒªã‚¹ãƒˆå½¢å¼ã«ãªã£ã¦ã„ã‚‹ã‚‚ã®ã‚’ã€æ”¹è¡Œã§çµåˆã™ã‚‹ã€‚
    """
    if isinstance(text, str):
        return text
    elif isinstance(text, list):
        return "\n".join(text)


def load_config(config, key, default=None, format=True):
    """
    configã‹ã‚‰keyã‚’èª­ã¿è¾¼ã‚€ã€‚å­˜åœ¨ã—ãªã‹ã£ãŸå ´åˆã¯ã€defaultå€¤ã‚’è¿”ã™.
    Args:
        config (dict): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸
        key (list[str]): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®keyã€‚æ·±ã„éšå±¤ã®ã‚‚ã®ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã€listã§æŒ‡å®šã™ã‚‹ã€‚strã‚’æŒ‡å®šã—ãŸå ´åˆã¯ã€ãã®ã¾ã¾keyã¨ã—ã¦æ‰±ã†ã€‚
        default (Any, optional): keyãŒå­˜åœ¨ã—ãªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤. Defaults to None.
        format (bool, optional): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ãŒãƒªã‚¹ãƒˆã®å ´åˆã€æ”¹è¡Œã§çµåˆã™ã‚‹ã‹ã©ã†ã‹. Defaults to True.
    Returns:
        str: keyã«å¯¾å¿œã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤
    """
    if isinstance(key, str):
        key = [key]
    item = config
    for k in key:
        if k in item:
            item = item[k]
        else:
            logger.info(
                f"load_config(): Couldn't find key in config. Use default. key={key}, value={default}"
            )
            return default
    logger.info(f"load_config(): {key}")
    if format:
        output = get_formatted_text(item)
    else:
        output = item
    logger.info(f"load_config(): Found key in config. key={key}, value={output}")
    return output


def convert_chat_history(chat_history: list[dict]):
    """
    chat_historyã‚’stringã«å¤‰æ›ã™ã‚‹ã€‚
    Args:
        chat_history (list[dict]): ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¸¡ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚"type"ã¨"message"ã®keyã‚’æŒã¤dictã®listã€‚
        "type"ã¯ã€"ai"ã¨"human"ã®2ç¨®é¡ã€‚
        https://www.notion.so/spiralai/BotChat-API-68aad0755564432d898e9fce9b5d21b9?pvs=4
    Returns:
        output (str): ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’stringã«å¤‰æ›ã—ãŸã‚‚ã®ã€‚"Human: XXX\nAssistant: XXX"ã¨ã„ã£ãŸå½¢å¼ã§è¿”ã™ã€‚
    """
    output = ""
    for chat in chat_history:
        if chat["type"] == "ai":
            output += "Assistant: " + chat["message"] + "\n"
        elif chat["type"] == "human":
            output += "Human: " + chat["message"] + "\n"
        else:
            raise ValueError(f"chat_historyã®typeãŒä¸æ­£ã§ã™ã€‚{chat['type']}")
    return output[:-1]  # æœ€å¾Œã®'\n'ã‚’å‰Šé™¤


def build_facts_string(searched_docs_meta: list):
    facts = ""
    for doc in searched_docs_meta:
        facts += "Reference Name: " + doc.resource_name + "  "
        if doc.resource_url:
            facts += "Reference URL: " + doc.resource_url + "  "
        facts += 'Reference Text: """' + doc.resource_text + '"""  '
    return facts


def build_annotation_string(
    annotation_pair_list: list[tuple[AnnotationMeta, AnnotationMeta]]
):
    annotations: str = ""
    for question, answer in annotation_pair_list:
        if question.description is None:
            continue
        annotations += "Sample Question: " + question.description + " \n"
        annotations += "Sample Answer: " + answer.description + " \n\n"

    return annotations


def build_llm_model(
    api_key,
    model_name,
    need_streaming=False,
    verbose=False,
    callback_manager=None,
    llm_model="openai",
):
    if callback_manager is None:
        need_streaming = False
        verbose = False

    if llm_model == "azure":
        llm = AzureChatOpenAI(
            openai_api_key=api_key,
            temperature=0.0,
            model_name=model_name,
            max_retries=3,
            request_timeout=60,
            streaming=need_streaming,
            verbose=verbose,
            callback_manager=callback_manager,
        )
    elif llm_model == "openai":
        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.0,
            model_name=model_name,
            max_retries=3,
            request_timeout=60,
            streaming=need_streaming,
            verbose=verbose,
            callback_manager=callback_manager,
        )
    return llm


if __name__ == "__main__":
    con = {
        # "body": '{"bot_id2": "7a42ed26-dc8f-46bc-9c4e-15306cc0279b", "user_id": "sample","created_at":1681706665,"posted_at":1681779536,"posted_by":"sample","message":"UPDATE3!!","updated_by":"annotatorID","updated_at":1681875192,"flag":false}'
        "body": '{"user_input": "æ—¥æœ¬ã®å†…é–£ç·ç†å¤§è‡£ã®åå‰ã¯ï¼Ÿ","company_id": "company_id","bot_id":"7a42ed26-dc8f-46bc-9c4e-15306cc0279b","updated_by":"anotator idddd","user_id":"sample","history":[{"type":"human","message":"æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ"},{"type":"ai","message":"ã‚ã‹ã‚Šã¾ã›ã‚“"}]}'
        # "body": '{"bot_id": "7a42ed26-dc8f-46bc-9c4e-15306cc0279b", "user_id": "sample","created_at":1681706665,"posted_at":1681779536,"posted_by":"sample","message":"UPDATE!!","updated_by":"annotatorID","flag":false}'
    }
    a = handler(con, {})
    print(a)

ä¸Šã®ã‚³ãƒ¼ãƒ‰ã§2å›ç›®ä»¥é™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé€ã‚Œãªããªã£ã¦ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ã«ä¿®æ­£ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ
answer: 2å›ç›®ä»¥é™ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé€ã‚Œãªããªã£ã¦ã„ã‚‹å•é¡Œã«å¯¾å‡¦ã™ã‚‹ã«ã¯ã€WebSocketã‚’ä»‹ã—ã¦ç¶™ç¶šçš„ã«ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã€ã‚³ãƒ¼ãƒ‰å…¨ä½“ã‚’å†æ§‹æˆã™ã‚‹ã‹ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›´ã‚’åŠ ãˆã¦ã‚µãƒ¼ãƒãƒ¼ã¨ã®WebSocketæ¥ç¶šã‚’ç¶­æŒã—ç¶šã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€ã“ã®è³ªå•ã«è¼‰ã›ã‚‰ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ã§ã¯ã€AWSã¾ãŸã¯ä»–ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¸Šã§ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã—ãŸè§£æ±ºæ–¹æ³•ã‚’ææ¡ˆã§ããªã„ãŸã‚ã€å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã“ã¨ãŒå›°é›£ã§ã™ã€‚

ä¸€èˆ¬çš„ãªæ–¹æ³•ã¨ã—ã¦ã¯ã€WebSocketã‚µãƒ¼ãƒãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒWebSocketã‚’ä»‹ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã—ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€å—ä¿¡ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã™ã€‚WebSocketã®æ¥ç¶šãŒç¢ºç«‹ã•ã‚ŒãŸã‚‰ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ã‚µãƒ¼ãƒãƒ¼ã«å¯¾ã—ã¦ä»»æ„ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã§ãã€ã‚µãƒ¼ãƒãƒ¼ã¯ãã‚Œã«å¯¾å¿œã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ–¹æ³•ã§ã¯ã€ç‰¹å®šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¿œç­”ã‚’å¾…ãŸãšã«ã€ä»»æ„ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã§ãã‚‹ãŸã‚ã€2å›ç›®ä»¥é™ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã†ã¾ãé€ã‚Œãªã„å•é¡Œã‚’è§£æ±ºã§ãã‚‹ã¯ãšã§ã™ã€‚

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã§æ—¢ã«WebSocketã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ¥ç¶šã‚’ç¶­æŒã—ç¶šã‘ã‚‹ã‚ˆã†ã«ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã§ã€ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€èª¤ã£ã¦æ¥ç¶šã‚’é–‰ã˜ã¦ã—ã¾ã‚ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚µãƒ¼ãƒãƒ¼é–“ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒé©åˆ‡ã«è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ãŸã ã—ã€å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ãŸã‚ã«ã¯ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã®è©³ç´°ã‚’æŠŠæ¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã©ã®ã‚ˆã†ãªæ–¹æ³•ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã‹ã€ã©ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã‹ãªã©ã€ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒã‚ã‚Œã°ã€å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
