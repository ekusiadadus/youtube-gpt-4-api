model: gpt-4
time: 0:03:32.194858
question: ```
import asyncio
import os
from typing import Any, Dict, List, Optional, Union

from langchain import LLMChain, PromptTemplate
from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler, BaseCallbackManager
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import AgentAction, AgentFinish, LLMResult

template = """Assistant is a large language model trained by OpenAI.

Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.

{history}
Human: {human_input}
Assistant:"""


class GenerateResponseType:
    human_input: str
    history: str
    text: str

    def __init__(self, human_input: str, history: str, text: str) -> None:
        self.human_input = human_input
        self.history = history
        self.text = text


class MyCustomCallbackHandler(AsyncCallbackHandler):
    def __init__(self, ws=None) -> None:
        super().__init__()
        self.ws = ws

    """Custom CallbackHandler."""

    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        pass

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        pass

    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """新しいtokenが来たらprintする"""
        print("\033[36m" + token + "\033[0m")
        # self.ws.send(token)

    async def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Do nothing."""
        pass

    async def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        """Print out that we are entering a chain."""
        class_name = serialized["name"]
        print(f"\n\n\033[1m> Entering new {class_name} chain...\033[0m")

    async def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Print out that we finished a chain."""
        print("\n\033[1m> Finished chain.\033[0m")

    async def on_chain_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Do nothing."""
        pass

    async def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any,
    ) -> None:
        """Do nothing."""
        pass

    async def on_agent_action(self, action: AgentAction, color: Optional[str] = None, **kwargs: Any) -> Any:
        """Run on agent action."""
        print(action)

    async def on_tool_end(
        self,
        output: str,
        color: Optional[str] = None,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """If not the final action, print out observation."""
        print(output)

    async def on_tool_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Do nothing."""
        pass

    async def on_text(
        self,
        text: str,
        color: Optional[str] = None,
        end: str = "",
        **kwargs: Optional[str],
    ) -> None:
        """Run when agent ends."""
        print(text)

    async def on_agent_finish(self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any) -> None:
        """Run on agent end."""
        print(finish.log)


class LangChainClient:
    def __init__(self, model_name: str, api_key: str, callbacks: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]] = None):
        self.model_name = model_name
        self.max_tokens = self._get_max_tokens(model_name)
        self.api_key = api_key
        self.prompt = prompt = PromptTemplate(input_variables=["history", "human_input"], template=template)
        self.callback_handler: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]] = callbacks

        self.llm = LLMChain(
            llm=ChatOpenAI(model=model_name, openai_api_key=api_key, client=None, streaming=True, callbacks=self.callback_handler),
            prompt=prompt,
            verbose=True,
            memory=ConversationBufferWindowMemory(k=2),
        )

    def _get_max_tokens(self, model_name: str) -> Optional[int]:
        model_token_limits = {
            "gpt-3.5-turbo": 4096,
            # Add more model token limits here if needed
        }
        return model_token_limits.get(model_name)

    async def generate_response(self, question: str, max_tokens: Optional[int] = None, temperature: float = 0.7, n: int = 1) -> str:
        if max_tokens is None:
            max_tokens = self.max_tokens

        response = self.llm(question)
        res = GenerateResponseType(**response)

        return res.text

    def add_callback(self, callback: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]]) -> None:
        self.callback_handler: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]] = callback
        self.llm = LLMChain(
            llm=ChatOpenAI(model=self.model_name, openai_api_key=self.api_key, client=None, streaming=True, callbacks=self.callback_handler),
            prompt=self.prompt,
            verbose=True,
            memory=ConversationBufferWindowMemory(k=2),
        )


async def main():
    client = LangChainClient(model_name="gpt-3.5-turbo", api_key=os.environ["OPENAI_API_KEY"])
    client.add_callback([MyCustomCallbackHandler()])
    response = await client.generate_response("What is the meaning of life?")
    print(response)


if __name__ == "__main__":
    asyncio.run(main())


```

```
/home/ekusiadadus/dev/minyoo/minyoo/.venv/lib/python3.11/site-packages/langchain/callbacks/manager.py:139: RuntimeWarning: coroutine 'AsyncCallbackHandler.on_chat_model_start' was never awaited
  getattr(handler, event_name)(*args, **kwargs)
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
/home/ekusiadadus/dev/minyoo/minyoo/.venv/lib/python3.11/site-packages/langchain/callbacks/manager.py:139: RuntimeWarning: coroutine 'MyCustomCallbackHandler.on_llm_new_token' was never awaited
  getattr(handler, event_name)(*args, **kwargs)
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
/home/ekusiadadus/dev/minyoo/minyoo/.venv/lib/python3.11/site-packages/langchain/callbacks/manager.py:139: RuntimeWarning: coroutine 'MyCustomCallbackHandler.on_llm_end' was never awaited
  getattr(handler, event_name)(*args, **kwargs)
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
```


await できていないとエラーが出ます。
どのように修正すればいいですか？
answer: エラーが発生しているのは、`AsyncCallbackHandler` の各メソッドが `await` されていないためです。`BaseCallbackManager` 内でイベントを処理する際に、`await` を使用して非同期のメソッドを呼び出すように修正する必要があります。

これを解決するには、以下の手順を実行してください。

1. まず、`langchain.callbacks.manager` から `BaseCallbackManager` をインポートしてください。これは、イベントを適切に処理するカスタムコールバックマネージャを定義するのに役立ちます。
```python
from langchain.callbacks.manager import BaseCallbackManager
```

2. 次に、`MyCustomCallbackManager` という名前の新しいクラスを作成し、`BaseCallbackManager` を継承します。

```python
class MyCustomCallbackManager(BaseCallbackManager):
    async def _run_method(self, handler, event_name, *args, **kwargs):
        method = getattr(handler, event_name, None)
        if method is not None:
            return await method(*args, **kwargs)
```

このカスタムコールバックマネージャは、親クラスである `BaseCallbackManager` からほとんど手を加えずに使用しますが、 `_run_method` 関数だけをオーバーライドしています。これは、イベントが適切に待機できるように、`await` を使用して各メソッドを呼び出すように修正されています。

3. 最後に、新しい `MyCustomCallbackManager` を使用して `LLMChain` を初期化します。

```python
self.llm = LLMChain(
    llm=ChatOpenAI(model=self.model_name, openai_api_key=self.api_key, client=None, streaming=True, callbacks=self.callback_handler),
    prompt=self.prompt,
    verbose=True,
    memory=ConversationBufferWindowMemory(k=2),
    manager=MyCustomCallbackManager(),  # ここに新しいカスタムコールバックマネージャを追加
)
```

これで、非同期メソッドが `await` され、警告が表示されなくなるはずです。

完全な修正後のコード例:

```python
import asyncio
import os
from typing import Any, Dict, List, Optional, Union

from langchain import LLMChain, PromptTemplate
from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler
from langchain.callbacks.manager import BaseCallbackManager
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import AgentAction, AgentFinish, LLMResult

template = """Assistant ... (same as before)"""

class GenerateResponseType:
  ... (same as before) 

class MyCustomCallbackHandler(AsyncCallbackHandler):
  ... (same as before)

# New custom callback manager
class MyCustomCallbackManager(BaseCallbackManager):
    async def _run_method(self, handler, event_name, *args, **kwargs):
        method = getattr(handler, event_name, None)
        if method is not None:
            return await method(*args, **kwargs)

class LangChainClient:
  ... (same as before, except the constructor and add_callback methods)

    def __init__(self, model_name: str, api_key: str, callbacks: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]] = None):
        ... (same as before)

        self.llm = LLMChain(
            llm=ChatOpenAI(model=model_name, openai_api_key=api_key, client=None, streaming=True, callbacks=self.callback_handler),
            prompt=prompt,
            verbose=True,
            memory=ConversationBufferWindowMemory(k=2),
            manager=MyCustomCallbackManager(),  # Add custom callback manager here
        )

    ... (same as before, except the add_callback method)

    def add_callback(self, callback: Optional[List[BaseCallbackHandler | AsyncCallbackHandler]]) -> None:
        self.callback_handler = callback
        self.llm = LLMChain(
            llm=ChatOpenAI(model=self.model_name, openai_api_key=self.api_key, client=None, streaming=True, callbacks=self.callback_handler),
            prompt=self.prompt,
            verbose=True,
            memory=ConversationBufferWindowMemory(k=2),
            manager=MyCustomCallbackManager(),  # Add custom callback manager here
        )

... (same as before, main function, etc)
```

これで修正が完了し、非同期コールバックが正しく機能するはずです。
