model: gpt-4
time: 0:01:53.609157
question: ```
from typing import Any, Dict, List, Optional, Union

from langchain.callbacks.base import CallbackManager, BaseCallbackHandler
from langchain.schema import AgentAction, AgentFinish, LLMResult

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

class MyCustomCallbackHandler(BaseCallbackHandler):
    """Custom CallbackHandler."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        pass

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        pass

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        '''新しいtokenが来たらprintする'''
        print('\033[36m' + token + '\033[0m')

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Do nothing."""
        pass

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Print out that we are entering a chain."""
        class_name = serialized["name"]
        print(f"\n\n\033[1m> Entering new {class_name} chain...\033[0m")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Print out that we finished a chain."""
        print("\n\033[1m> Finished chain.\033[0m")

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Do nothing."""
        pass

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any,
    ) -> None:
        """Do nothing."""
        pass

    def on_agent_action(
        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any
    ) -> Any:
        """Run on agent action."""
        print(action)

    def on_tool_end(
        self,
        output: str,
        color: Optional[str] = None,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """If not the final action, print out observation."""
        print(output)

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Do nothing."""
        pass

    def on_text(
        self,
        text: str,
        color: Optional[str] = None,
        end: str = "",
        **kwargs: Optional[str],
    ) -> None:
        """Run when agent ends."""
        print(text)

    def on_agent_finish(
        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any
    ) -> None:
        """Run on agent end."""
        print(finish.log)

import key, os

os.environ["OPENAI_API_KEY"] = key.OPEN_API_KEY

llm = ChatOpenAI(streaming=True, callback_manager=CallbackManager([MyCustomCallbackHandler()]), verbose=True, temperature=0)
print(llm(messages=[HumanMessage(content="炭酸水についての歌詞を書いてください。")]))
```

上は、Callback を使うサンプルコードです。

```src/infra/client/langchain/langchain_client.py
class GenerateResponseType:
    human_input: str
    history: str
    text: str

    def __init__(self, human_input: str, history: str, text: str) -> None:
        self.human_input = human_input
        self.history = history
        self.text = text


class WebsocketCallbackClient(BaseCallbackHandler):
    def __init__(self, ws_client) -> None:
        self.ws_client = ws_client


class MyCustomCallbackHandler(BaseCallbackHandler):
    """Custom CallbackHandler."""

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLM の処理開始。prompt の内容を出力"""
        pass

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM の処理終了。何もしない"""
        pass

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """LLM から新しい Token が出力。いわゆる Streaming の部分"""
        pass

    def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """LLM の処理中にエラーが発生"""
        pass

    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        """Chain の処理がスタート"""
        pass

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Chain の処理が終了"""
        pass

    def on_chain_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Chain の実行でエラーが発生"""
        pass

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any,
    ) -> None:
        """Tool の実行が開始"""
        pass

    def on_agent_action(self, action: AgentAction, color: Optional[str] = None, **kwargs: Any) -> Any:
        """Agent がアクションを実施。Agent の Streaming は大体ここ"""
        print(action)

    def on_tool_end(
        self,
        output: str,
        color: Optional[str] = None,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """Tool の使用が終了。Final Answer でなければ[Observation]が出力"""
        print(output)

    def on_tool_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """Tool の使用でエラーが発生"""
        pass

    def on_text(
        self,
        text: str,
        color: Optional[str] = None,
        end: str = "",
        **kwargs: Optional[str],
    ) -> None:
        """Agent の終了時に呼び出される。完全に終了したとき（？）。結果の出力"""
        print(text)

    def on_agent_finish(self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any) -> None:
        """Agent が終了した時に呼び出される。ログの出力"""
        print(finish.log)


class LangChainClient:
    def __init__(self, model_name: str, api_key: str):
        self.model_name = model_name
        self.max_tokens = self._get_max_tokens(model_name)
        self.api_key = api_key
        self.prompt = prompt = PromptTemplate(input_variables=["history", "human_input"], template=template)
        self.ws = websocket.WebSocket()
        self.llm = ChatOpenAI(model=model_name, openai_api_key=api_key, max_retries=3, client=None, streaming=True, verbose=False)

        self.client = LLMChain(
            llm=self.llm,
            prompt=prompt,
            verbose=True,
            memory=ConversationBufferWindowMemory(k=2),
        )

    def _get_max_tokens(self, model_name: str) -> Optional[int]:
        model_token_limits = {
            "gpt-3.5-turbo": 4096,
            # Add more model token limits here if needed
        }
        return model_token_limits.get(model_name)

```

```src/web/init.py
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}

    async def connect(self, websocket: WebSocket, bot_id: str):
        if bot_id in self.active_connections:
            self.active_connections[bot_id].append(websocket)
        else:
            self.active_connections[bot_id] = [websocket]
        await websocket.accept()

    async def disconnect(self, websocket: WebSocket, bot_id: str):
        self.active_connections[bot_id].remove(websocket)

    async def send_message(self, message: str, bot_id: str):
        for connection in self.active_connections[bot_id]:
            await connection.send_text(message)

app = FastAPI(title="minyoo")
router = APIRouter()
origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://localhost:8080",
    "http://localhost:3000",
    "http://localhost:7999",
    "*",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)
manager = ConnectionManager()


@app.websocket("/ws/chat/{user_id}/{bot_id}", name="chat")
async def websocket_endpoint(websocket: WebSocket, user_id: str, bot_id: str, server: Server = Depends(get_server)):
    await manager.connect(websocket, bot_id)
    try:
        while True:
            data = await websocket.receive_text()
            res = server.chat_handler.web_socket_chat(data)
            print(res)
            await manager.send_message(f"ボットID: {bot_id} ユーザーID: {user_id} メッセージ: {res}", bot_id)
    except WebSocketDisconnect:
        await manager.disconnect(websocket, bot_id)
```

ステップバイステップで教えてください。
src/infra/client/langchain/langchain_client.py に、websocket での通信を行うクライアントを実装しています。
上のコードをもとに、src/infra/client/langchain/langchain_client.py, src/web/init.py でユーザーと、LangchainClientのWebsocket通信ができるようにコードを完成させてください。
answer: 以下のステップを実行することで、ユーザーとLangchainClientのWebsocket通信ができるようになります。

1. src/infra/client/langchain/langchain_client.pyの`WebsocketCallbackClient`クラスを完成させるために、各コールバックメソッドでWebSocketインスタンスを使用してレスポンスを送信します。

```python
class WebsocketCallbackClient(BaseCallbackHandler):
    def __init__(self, ws_client) -> None:
        self.ws_client = ws_client

   # 以下の各コールバックメソッドで、レスポンスをWebSocket経由で送信できるようにします。
```

2. `LangChainClient` クラスに、WebSocketインスタンスを渡すための新しい引数を追加し、`WebsocketCallbackClient` クラスをインスタンス化し、コールバックリストに追加します。

```python
class LangChainClient:
    def __init__(self, model_name: str, api_key: str, websocket: WebSocket):
        self.model_name = model_name
        self.max_tokens = self._get_max_tokens(model_name)
        self.api_key = api_key
        self.prompt = PromptTemplate(input_variables=["history", "human_input"],
                                      template=template)
        self.websocket = websocket
        self.ws_callback_client = WebsocketCallbackClient(self.websocket)
        
        self.llm = ChatOpenAI(
            model=model_name, openai_api_key=api_key, max_retries=3, client=None,
            streaming=True, verbose=False,
            callback_manager=CallbackManager([MyCustomCallbackHandler(), self.ws_callback_client])
        )
   # ...
```

3. src/web/init.py内の`websocket_endpoint`関数で、LangChainClientインスタンスを作成する際にWebSocketインスタンスを渡すようにします。

```python
async def websocket_endpoint(websocket: WebSocket, user_id: str, bot_id: str,
                             server: Server = Depends(get_server)):
    await manager.connect(websocket, bot_id)
    langchain_client = LangChainClient("gpt-3.5-turbo", "your_api_key", websocket)  # ここでWebSocketインスタンスを渡す
    try:
        while True:
            data = await websocket.receive_text()
            res = server.chat_handler.web_socket_chat(data)
            print(res)
            await manager.send_message(f"ボットID: {bot_id} ユーザーID: {user_id} メッセージ: {res}", bot_id)
    except WebSocketDisconnect:
        await manager.disconnect(websocket, bot_id)
```

4. 必要に応じて、`MyCustomCallbackHandler` クラスと `WebsocketCallbackClient` クラスのコールバックメソッドをクライアントとサーバーが互いにコミュニケーションできるように調整します。たとえば、`on_llm_new_token`メソッドで出力をクライアントに送信する場合は、`WebsocketCallbackClient` クラス内で次のように実装します。

```python
class WebsocketCallbackClient(BaseCallbackHandler):
    # ... (他のコード)

    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        await self.ws_client.send_text(token)
```

これで、ユーザーとLangchainClient間のWebsocket通信ができるようになります。なお、この実装例はあくまで一例であり、他の方法や設計でも実現できる可能性があることに注意してください。
