model: gpt-4
time: 0:00:25.209344
question: ```
from __future__ import annotations
import json
import os
import logging
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor

try:
    from decorators import handle_exception, notify_handler_call

    from params import Params
    from crypter import decrypt
    from models import HistoryMeta, ResourceType, ChatHistory, AnnotationMeta
    from repository import PromptSettingClient
    from repository.vector_db import VectorDBClient
    from repository.repository import (
        ResourceMetaRepository,
        ChatHistoryRepo,
        ResourceMetaClient,
        ChatHistoryClient,
    )

except Exception:
    import sys

    sys.path.append("amplify/backend/function/kittbackenddevUtilLayer/lib/python")
    from decorators import handle_exception, notify_handler_call

    from params import Params
    from crypter import decrypt
    from models import HistoryMeta, ResourceType, ChatHistory, AnnotationMeta
    from repository import PromptSettingClient
    from repository.vector_db import VectorDBClient
    from repository.repository import (
        ResourceMetaRepository,
        ChatHistoryRepo,
        ResourceMetaClient,
        ChatHistoryClient,
    )

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

env = os.environ["ENV"]

# Retryã§å›å¾©ãŒè¦‹è¾¼ã¾ã‚Œã‚‹ã‚¨ãƒ©ãƒ¼ç¾¤
from openai.error import (
    Timeout,
    APIError,
    APIConnectionError,
    RateLimitError,
    ServiceUnavailableError,
)


class MissingBotAPIKey(Exception):
    pass


from langchain.chains import LLMChain
from langchain.prompts.prompt import PromptTemplate
from langchain.chat_models import ChatOpenAI, AzureChatOpenAI

from typing import Any, Dict, List, Union

import boto3
from langchain.schema import AgentAction, AgentFinish, LLMResult

from langchain.callbacks.base import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler

RATE_LIMIT_ERR_MSG = "RateLimitä¸Šé™ã«åˆ°é”ã—ã¦ã„ã¾ã™ã€‚ å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
LLM_API_ERR_MSG = "LLM Server ã®ErrorãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
SERVER_ERR_MSG = "InternelServer ErrorãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
MISSING_BOT_API_KEY_MSG = "Botã«APIKeyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"


class WebsocketChatClient:
    def __init__(self, connection_id: str, domain_name: str, stage: str):
        self.connection_id = connection_id
        self.domain_name = domain_name
        self.stage = stage
        self.client = boto3.client(
            "apigatewaymanagementapi", endpoint_url=f"https://{domain_name}/{stage}"
        )
        self._closed = False

    def send_msg(self, msg: str):
        data = json.dumps({"type": "msg", "message": msg})
        self.client.post_to_connection(Data=data, ConnectionId=self.connection_id)

    def send_err(self, err_name, err_msg: str):
        data = json.dumps({"type": "err", "name": err_name, "message": err_msg})
        self.client.post_to_connection(Data=data, ConnectionId=self.connection_id)

    def send_end(self, data: dict):
        data["type"] = "end"
        self.client.post_to_connection(
            Data=json.dumps(data), ConnectionId=self.connection_id
        )
        self.close()

    def close(self):
        if not self._closed:
            # delete connection ãŒå…ˆ
            self.client.delete_connection(ConnectionId=self.connection_id)
            self.client.close()
            self._closed = True


class WebsocketCallbackHandler(BaseCallbackHandler):
    """callbak handler for aws websocket"""

    def __init__(
        self,
        ws_client: any,  # WebsocketChatClient
        referred_docs: list,
        splitter=None,
    ) -> None:
        super().__init__()
        self.client = ws_client
        self.referred_docs = referred_docs
        self.splitter = splitter

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        if self.splitter:
            token = self.splitter.apply_stream(token)
            if token == "":
                return
        self.client.send_msg(token)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        data = {"documents": self.referred_docs}
        self.client.send_end(data)
        self.client.close()

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when LLM errors."""
        """
        ãƒ¡ãƒ¢:
        LangChainã®retryã‚’è¡Œã‚ã›ãšã«ã“ã“ã§ retry ã‚’è‡ªå‰ã§è¡Œã£ã¦ã€
        error => retry ã®åº¦ã«ãƒ¦ãƒ¼ã‚¶ã¸ç¾çŠ¶ã‚’é€šçŸ¥ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹
        """

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Run when tool starts running."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""
        pass

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> None:
        """Run on arbitrary text."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent finish."""


def get_websocket_callback_manager(
    ws_client: any,  # WebsocketChatClient
    referred_docs: list,
    splitter=None,
):
    """Callback manager with its own handler for AWS Websocket API."""
    cbHandler = WebsocketCallbackHandler(
        ws_client=ws_client,
        referred_docs=referred_docs,
        splitter=splitter,
    )
    return CallbackManager([cbHandler])


pinecone_api_key = Params().get_parameter(os.environ["PINECONE_API_KEY"])

resource_meta_client = ResourceMetaClient(env=env)
resource_meta_repository = ResourceMetaRepository(resource_client=resource_meta_client)
bot_repository = resource_meta_repository  # é«˜é€ŸåŒ–ã®ãŸã‚ã«æ—¢å­˜ã®ã‚‚ã®ã‚’å‚ç…§æ¸¡ã—
history_meta_repository = resource_meta_repository  # é«˜é€ŸåŒ–ã®ãŸã‚ã«æ—¢å­˜ã®ã‚‚ã®ã‚’å‚ç…§æ¸¡ã—
prompts_client = PromptSettingClient(env)
chat_history_repository = ChatHistoryRepo(ChatHistoryClient(env))


# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®šç¾©
_template = """ä»¥ä¸‹ã® Chat History ã‚’è¸ã¾ãˆã¦ã€Follow Up Input ã‚’ Standalone question ã«è¨€ã„ç›´ã—ã¦ãã ã•ã„ã€‚

====
Chat History:
{chat_history}
====
Follow Up Input: {question}
Standalone question:"""
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)


# @handle_exception(logger)
@notify_handler_call(logger)
def handler(event, context):
    body = (
        json.loads(event["body"]) if isinstance(event["body"], str) else event["body"]
    )
    bot_id = body["bot_id"]
    return post_handler(event, bot_id, body)


# ã“ã‚Œã¯ãªãœé–¢æ•°åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã‹
def put_history_meta(history_meta: HistoryMeta):
    return history_meta_repository.put(history_meta)


# ã“ã‚Œã¯ãªãœé–¢æ•°åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã‹
def put_chat_history(chat_history: ChatHistory):
    return chat_history_repository.put(chat_history)


class Splitter:
    """
    Streamã•ã‚Œã¦ãã‚‹æ–‡å­—åˆ—ã®ä¸­ã§ã€ç‰¹å®šã®æ–‡å­—åˆ—ã‚’è¦‹ã¤ã‘ã¦ã€å‡ºåŠ›ã®åˆ¶å¾¡ã‚’è¡Œã†ã€‚
    GPTã®å‡ºåŠ›ãŒä¸å®‰å®šã§ã€ç‰¹å®šã®æ–‡å­—åˆ—ãŒå‡ºåŠ›ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ãã®å ´åˆã¯safe_close()ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§å…¨æ–‡ã‚’å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
    """

    def __init__(self, split_conditions):
        """
        Args:
            split_conditions (list): å‡ºåŠ›é–‹å§‹ã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã€‚ã©ã‚Œã‹ä¸€ã¤ã§ã‚‚å‡ºç¾ã—ãŸã‚‰ã€å‡ºåŠ›ã‚’é–‹å§‹ã™ã‚‹ã€‚
            ç©ºã®å ´åˆã¯ã€Splitterè‡ªä½“ãŒç„¡åŠ¹åŒ–ã•ã‚Œã€ã™ã¹ã¦ã®æ–‡å­—åˆ—ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã€‚
        """
        self.conditions = split_conditions
        self.text = ""
        self.output = ""
        if split_conditions:
            self.flag = False  # ä½•ã‚‰ã‹ã®æ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã¨ãã«ã¯ã€æ–‡å­—ã‚’æ¨ã¦ã‚‹
        else:
            self.flag = True  # æ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã¨ãã¯ã€å¸¸ã«å‡ºåŠ›

    def safe_close(self):
        """
        ã‚‚ã—ä½•ã‚‚å‡ºåŠ›ã•ã‚Œã¦ã„ãªã‹ã£ãŸå ´åˆã€å…¥åŠ›ã•ã‚ŒãŸå…¨æ–‡ã‚’è¿”ã™ã€‚
        å‡ºåŠ›ã•ã‚Œã¦ã„ãŸå ´åˆã¯ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã™ã®ã§ã€apply_streamã®å¾Œã«ã“ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã›ã°ã€å¸¸ã«æ­£ã—ã„æ–‡ç« ã‚’å–å¾—ã§ãã‚‹ã€‚
        """
        if self.output == "":
            return self.text
        else:
            return ""

    def apply(self, text):
        """
        Streamã§ã¯ãªãã¦ã€ä¸€æ°—ã«æ–‡å­—åˆ—ã‚’æ¸¡ã™å ´åˆã«ä½¿ã†ã€‚
        Args:
            text (str): å…¥åŠ›æ–‡å­—åˆ—
        Returns:
            str: å‡ºåŠ›æ–‡å­—åˆ—
        """
        result = ""
        for c in text:
            result += self.apply_stream(c)
        return result + self.safe_close()

    def apply_stream(self, token):
        """
        Streamã§æ–‡å­—ã‚’æ¸¡ã™å ´åˆã«ä½¿ã†ã€‚
        Args:
            token (str): å…¥åŠ›æ–‡å­—
        Returns:
            str: å‡ºåŠ›æ–‡å­—ã€‚conditionãŒæº€ãŸã•ã‚Œã¦ã„ãªã„é–“ã¯ã€ç©ºæ–‡å­—åˆ—ãŒè¿”ã•ã‚Œã‚‹ã€‚
        """

        # do_output = copy.deepcopy(self.flag)
        discard_this = False

        # ãƒ•ãƒ©ã‚°ã®åˆ¶å¾¡
        for tgt in self.conditions:
            f1 = tgt in self.text
            f2 = tgt in (self.text + token)
            if (not f1) and f2:
                self.flag = True
                discard_this = True  # ã“ã®æ–‡å­—ã¾ã§ã¯å‡ºåŠ›ã—ãªã„
        self.text += token

        # å‡ºåŠ›ã®åˆ¶å¾¡
        if self.flag:
            if discard_this:
                return ""
            elif token in [" "] and self.output == "":
                return ""
            else:
                self.output += token
                return token
        else:
            return ""


def post_handler(event, bot_id: str, request_body: dict):
    try:
        timer = OrderedDict()
        timer["start"] = time.time()

        connectionId = event.get("requestContext", {}).get("connectionId")
        domain_name = event.get("requestContext", {}).get("domainName")
        stage = event.get("requestContext", {}).get("stage")
        # ãƒ¦ãƒ¼ã‚¶ã¯æ—¢ã«WebSocketã¨ç¹‹ã„ã§ã„ã‚‹ã®ã§, close() ã§ãã‚‹ã‚ˆã† client ã‚’ã¾ãšå–å¾—ã™ã‚‹
        timer["parse_parameters"] = time.time()
        ws_client = WebsocketChatClient(connectionId, domain_name, stage)

        company_id = request_body["company_id"]
        user_id = request_body["user_id"]
        user_input = request_body["user_input"]
        user_posted_at = int(time.time())

        logger.info(
            "ğŸ‘£ post_handler\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # user_posted_at: {user_posted_at}\n"
            + f" # domain_name: {domain_name}\n"
            + f" # stage: {stage}\n"
            + f" # company_id: {company_id}\n"
            + f" # user_id: {user_id}\n\n"
            + f" # user_input:\n{user_input}\n\n"
            + f" # request_body:\n{request_body}\n\n"
        )

        timer["post_handler"] = time.time()

        bot = bot_repository.get(  # api_keyå–å¾—ã®ãŸã‚ã«botã®æƒ…å ±ã‚’å–å¾—
            ResourceType.bot, company_id, bot_id
        )

        if not bot.llm_api_key:
            logger.error(
                f"LLM API Key hasn't been set for the bot '{bot.resource_name}'"
            )
            raise MissingBotAPIKey()

        api_key = bot.llm_api_key
        logger.info(f"llm_api_key obtained: {api_key}")

        logger.info(
            "ğŸ‘£ load_bot\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # bot_id: {bot_id}\n\n"
            + f" # bot:\n{bot.to_dict()}\n\n"
        )
        timer["load_bot"] = time.time()

        # prep repository
        resource_meta_repository = ResourceMetaRepository(
            resource_client=resource_meta_client,
            vector_client=VectorDBClient(
                api_key=pinecone_api_key,
                env=env,
                openai_api_key=api_key,
            ),
        )
        timer["resource_meta_repository"] = time.time()

        try:
            prompt_config_id = bot.prompt
            prompt_config = json.loads(
                prompts_client.get_default(prompt_config_id + ".json")
            )
        except Exception as err:
            logger.info(
                f"error loading prompt: {bot.prompt}. use default prompt. Error: {str(err)}"
            )

            prompt_config_id = "example"
            prompt_config = json.loads(
                prompts_client.get_default(prompt_config_id + ".json")
            )
        # logger.info(f"prompt_config: {prompt_config}")

        qa_prompt_with_character_text = load_config(
            prompt_config,
            key=["PromptSettings", "UserPrompt", "Prompt"],
            default=None,
            format=True,
        )
        qa_prompt_with_character = PromptTemplate(
            template=qa_prompt_with_character_text,
            input_variables=["context", "question"],
        )

        logger.info(
            "ğŸ‘£ load_prompt\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # bot.prompt: {bot.prompt}\n"
            + f" # prompt_config_id: {prompt_config_id}\n\n"
            + f" # prompt_config:\n{prompt_config}\n\n"
            + f" # qa_prompt_with_character_text:\n{qa_prompt_with_character_text}\n\n"
        )
        timer["load_prompt"] = time.time()

        # LLMã®ä¿®æ­£å±¥æ­´ã‚’å–å¾— (ç¾åœ¨ã¯ã¾ã ä½¿ã‚ãªã„ã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã‚‹)
        # annotations_pairs = resource_meta_repository.search_annotation_pairs(company_id, user_input, annos_refs)
        # annotations = build_annotation_string(annotations_pairs)
        # logger.info(annotations)

        # LLMãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
        llm_model = "openai"  # Azureã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸå ´åˆã¯ã€ã“ã“ã‚’å‹•çš„ã«å¤‰æ›´
        model_name = "gpt-3.5-turbo"  # å°†æ¥ã€ã“ã“ã‚‚å‹•çš„ã«å¤‰æ›´

        # Define tokenizer
        if model_name == "gpt-3.5-turbo":
            max_tokens = 4096 - 512  # 512ã¯å›ç­”ã®ãŸã‚ã®ä½™ç™½
        elif model_name == "gpt-4":
            max_tokens = 8192 - 512  # 512ã¯å›ç­”ã®ãŸã‚ã®ä½™ç™½
        else:
            raise NotImplementedError(f"Invalid model_name: {model_name}")

        # ã“ã“ã‹ã‚‰ã€LangChainã‚’å‘¼ã³å‡ºã—

        ###############################################
        # Step1: éå»ã®è³ªå•å±¥æ­´ã‚’çµ±åˆã—ã¦ã€å˜ä¸€ã®è³ªå•ã«ã™ã‚‹
        # Greeting PromptãŒã‚ã‚‹å ´åˆã¯ç„¡è¦–ã€‚å°‘ãªãã¨ã‚‚ä¸€å¯¾ã®QAãŒã‚ã‚‹å ´åˆã«condenseã™ã‚‹ã€‚

        chat_history = request_body["history"]
        user_question = request_body["user_input"]
        max_history_length = None
        num_tokens = None
        trial_prompt = None

        # è³ªå•ç”Ÿæˆ chain ã‚’ä½œæˆ
        llm_without_streaming = build_llm_model(
            api_key=api_key,
            model_name=model_name,
            need_streaming=False,
            verbose=True,
            llm_model=llm_model,
        )

        if len(chat_history) >= 2:
            chain_condense_question = LLMChain(
                llm=llm_without_streaming,
                prompt=CONDENSE_QUESTION_PROMPT,
                verbose=True,
            )
            # è³ªå•å±¥æ­´ã‚‚å«ã‚ã¦ã€åˆè¨ˆtokenæ•°ãŒmax_tokensã‚’è¶…ãˆãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚ã¾ãŸã€ã„ãšã‚Œã«ã›ã‚ˆ6å›ä»¥ä¸Šã®è³ªå•å±¥æ­´ã¯ç„¡è¦–ã™ã‚‹ã€‚
            # ãŸã£ãŸ1ä»¶ã§ã‚‚contexté•·ã‚’è¶…ãˆã¦ã—ã¾ã†å ´åˆã¯ã€ã€å…ƒã®è³ªå•ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
            for history_length in range(min(6, len(chat_history)), 0, -2):
                trial_prompt = CONDENSE_QUESTION_PROMPT.format(
                    chat_history=convert_chat_history(chat_history[-history_length:]),
                    question=user_question,
                )
                num_tokens = 0  # len(tokenizer.encode(trial_prompt))

                if num_tokens <= max_tokens:
                    max_history_length = history_length
                    break
                else:
                    continue

            if max_history_length is not None:
                condensed_question = chain_condense_question.run(
                    chat_history=convert_chat_history(
                        chat_history[-max_history_length:]
                    ),
                    question=user_question,
                    verbose=True,
                )
            else:
                condensed_question = user_question
        else:
            condensed_question = user_question

        # logger.info(convert_chat_history(chat_history))

        logger.info(
            "ğŸ‘£ chain_condense_question\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # max_history_length: {max_history_length}\n"
            + f" # max_tokens: {max_tokens}\n"
            + f" # num_tokens: {num_tokens}\n\n"
            + f" # chat_history:\n{chat_history}\n\n"
            + f" # user_question:\n{user_question}\n\n"
            + f" # condensed_question:\n{condensed_question}\n\n"
            + f" # entire prompt:\n{trial_prompt}\n\n"
        )
        timer["chain_condense_question"] = time.time()

        ###############################################
        # Step2: VectorDBã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å–å¾—ã™ã‚‹
        # vector-search docs for user_input
        references: list[dict[str, str]] = bot.references
        # logger.info(f"references: {references}")
        docs_refs: list[str] = [
            di["resource_id"]
            for di in list(
                filter(
                    lambda item: item.get("resource_type") == "docs",
                    references,
                )
            )
        ]
        # annos_refs: list[str] = [
        #    f"annotation#{di['resource_id']}"
        #    for di in list(
        #        filter(
        #            lambda item: item.get("resource_type") == "annotation", references  # noqa
        #        )
        #    )
        # ]
        searched_docs_meta = resource_meta_repository.search_documents(
            company_id,
            condensed_question,
            docs_refs,
        )

        # æœ€å¤§tokenæ•°ä¸Šé™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«é¸å®š
        facts = build_facts_string(searched_docs_meta)

        logger.info(
            "ğŸ‘£ search_docs\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # condensed_question: {condensed_question}\n"
            + f" # max_tokens: {max_tokens}\n"
            + f" # num_tokens: {num_tokens}\n"
            + f" # docs_refs:\n{docs_refs}\n\n"
            + f" # searched_docs_meta:\n{searched_docs_meta}\n\n"
            + f" # facts:\n{facts}\n\n"
        )
        timer["search_docs"] = time.time()

        ###############################################
        # Step3: QAã‚’å®Ÿæ–½ã™ã‚‹ã€‚

        # å‚ç…§ã—ãŸ Document ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        referred_docs = docs_to_referred_sources(searched_docs_meta)
        # websocketç”¨ã® callback handler ã‚’ç”Ÿæˆ
        callback_manager = get_websocket_callback_manager(
            ws_client=ws_client,
            referred_docs=referred_docs,
            splitter=Splitter(
                prompt_config["PromptSettings"]["UserPrompt"]["Splitter"]
            ),
        )
        timer["callback_manager"] = time.time()
        # QAå›ç­”ã‚’ã™ã‚‹ chain ã‚’ä½œæˆ
        llm_with_streaming = build_llm_model(
            api_key=api_key,
            model_name=model_name,
            need_streaming=True,
            verbose=True,
            callback_manager=callback_manager,
            llm_model=llm_model,
        )
        timer["llm_with_streaming"] = time.time()

        chain_factorial_qa_with_character = LLMChain(
            llm=llm_with_streaming,
            prompt=qa_prompt_with_character,
            verbose=False,
        )
        timer["chain_factorial_qa_with_character"] = time.time()
        answer_with_character = chain_factorial_qa_with_character.run(
            context=facts,
            question=condensed_question,
        )
        logger.info(
            "ğŸ‘£ qa_prompt_with_character\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # condensed_question:\n{condensed_question}\n\n"
            + f" # answer_with_character: {answer_with_character}\n\n"
            + f" # facts:\n{facts}\n\n"
        )
        timer["qa_prompt_with_character"] = time.time()

        final_response = answer_with_character

        ###############################################
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜
        # humanã‹ã‚‰ã®å•ã„ã‹ã‘ã®ä¿å­˜
        chat_history_user = ChatHistory.create(
            # user_inputã®è¨˜éŒ²,updatedæƒ…å ±ã¯ä¸è¦,chain.runã™ã‚‹å‰ã«è¨˜éŒ²
            bot_id=bot_id,
            user_id=user_id,
            created_at=request_body["created_at"]
            if "created_at" in request_body
            else 1681706665,
            posted_at=user_posted_at,
            posted_by=user_id,
            message=user_input,
        )

        # logger.info(f"chat_history: {chat_history_user.to_dict()}")
        # put_chat_history(chat_history=chat_history_user)

        # aiã‹ã‚‰ã®å›ç­”ã®ä¿å­˜
        chat_history_bot = ChatHistory.create(  # botã®respã®è¨˜éŒ²,updatedæƒ…å ±ã¯ä¸è¦
            bot_id=bot_id,
            user_id=user_id,
            created_at=request_body["created_at"]
            if "created_at" in request_body
            else 1681706665,
            posted_at=int(time.time()),
            posted_by=bot_id,
            message=final_response,
        )

        # logger.info(f"chat_history: {chat_history_bot.to_dict()}")
        # put_chat_history(chat_history=chat_history_bot)

        logger.info("Chat history has been put.")

        # history_metaã®ä¿å­˜
        history_meta = HistoryMeta.create(
            company_id=company_id,
            resource_id=connectionId,
            resource_name=connectionId,
            created_by=user_id,
            updated_by=user_id,
            description=final_response,
        )

        # logger.info(f"history_meta: {history_meta.to_dict()}")
        # put_history_meta(history_meta=history_meta)

        logger.info(
            "ğŸ‘£ put_chat_history\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # chat_history(user):\n{chat_history_user.to_dict()}\n\n"
            + f" # chat_history(bot):\n{chat_history_bot.to_dict()}\n\n"
            + f" # history_meta:\n{history_meta.to_dict()}\n\n"
        )
        timer["put_chat_history"] = time.time()

        # timerã‚’ä¿å­˜
        timer_diff = OrderedDict()
        for k in timer:
            timer_diff[k] = timer[k] - timer["start"]
        logger.info(
            "â±ï¸ timer\n\n"
            + f" # connectionId: {connectionId}\n\n"
            + f" # timer_diff:\n{json.dumps(timer_diff, indent=4)}\n\n"
            + f" # timer:\n{json.dumps(timer, indent=4)}\n\n"
        )

        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                async_put, history_meta, chat_history_user, chat_history_bot
            )
            future.result()

        return {"statusCode": 200}
    except MissingBotAPIKey as e:
        # é€šå¸¸ç™ºç”ŸãŒæƒ³å®šã•ã‚Œã‚‹ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ info
        logger.info(f"Error: {e}")
        ws_client.send_err("MissingBotAPIKey", MISSING_BOT_API_KEY_MSG)
        ws_client.close()
        return {"statusCode": 200}
    except RateLimitError as e:
        # ç¾çŠ¶ã® max_retry=3 ã ã¨ã“ã“ã¾ã§ã¯æ»…å¤šã«æ¥ãªã„

        logger.info(f"Error: {e}")
        logger.info(f"Message: {str(e)}")
        ws_client.send_err(
            "RateLimit",
            RATE_LIMIT_ERR_MSG,
        )
        ws_client.close()
        return {"statusCode": 200}
    except (APIConnectionError, APIError, ServiceUnavailableError, Timeout) as e:
        # é€šå¸¸ç™ºç”ŸãŒæƒ³å®šã•ã‚Œã‚‹ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ info
        logger.info(f"Error: {e}")
        logger.info(f"Message: {str(e)}")
        ws_client.send_err(
            "LLMAPI",
            LLM_API_ERR_MSG,
        )
        ws_client.close()
        return {"statusCode": 200}
    except Exception as e:
        logger.error(f"Error: {e}")
        logger.error(f"Message: {str(e)}")
        ws_client.send_err(
            "InternalServer",
            SERVER_ERR_MSG,
        )
        ws_client.close()
        return {
            "statusCode": 500,
            "headers": {
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "OPTIONS,POST,GET",
                "Content-Type": "application/json",
            },
            "body": json.dumps({"Error": str(e)}),
        }


def get_formatted_text(text):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã ã¨ãã«ã€ãƒªã‚¹ãƒˆå½¢å¼ã«ãªã£ã¦ã„ã‚‹ã‚‚ã®ã‚’ã€æ”¹è¡Œã§çµåˆã™ã‚‹ã€‚
    """
    if isinstance(text, str):
        return text
    elif isinstance(text, list):
        return "\n".join(text)


def load_config(config, key, default=None, format=True):
    """
    configã‹ã‚‰keyã‚’èª­ã¿è¾¼ã‚€ã€‚å­˜åœ¨ã—ãªã‹ã£ãŸå ´åˆã¯ã€defaultå€¤ã‚’è¿”ã™.
    Args:
        config (dict): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸
        key (list[str]): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®keyã€‚æ·±ã„éšå±¤ã®ã‚‚ã®ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã€listã§æŒ‡å®šã™ã‚‹ã€‚strã‚’æŒ‡å®šã—ãŸå ´åˆã¯ã€ãã®ã¾ã¾keyã¨ã—ã¦æ‰±ã†ã€‚
        default (Any, optional): keyãŒå­˜åœ¨ã—ãªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤. Defaults to None.
        format (bool, optional): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ãŒãƒªã‚¹ãƒˆã®å ´åˆã€æ”¹è¡Œã§çµåˆã™ã‚‹ã‹ã©ã†ã‹. Defaults to True.
    Returns:
        str: keyã«å¯¾å¿œã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤
    """
    if isinstance(key, str):
        key = [key]
    item = config
    for k in key:
        if k in item:
            item = item[k]
        else:
            logger.info(
                f"load_config(): Couldn't find key in config. Use default. key={key}, value={default}"
            )
            return default
    logger.info(f"load_config(): {key}")
    if format:
        output = get_formatted_text(item)
    else:
        output = item
    logger.info(f"load_config(): Found key in config. key={key}, value={output}")
    return output


def convert_chat_history(chat_history: list[dict]):
    """
    chat_historyã‚’stringã«å¤‰æ›ã™ã‚‹ã€‚
    Args:
        chat_history (list[dict]): ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ¸¡ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚"type"ã¨"message"ã®keyã‚’æŒã¤dictã®listã€‚
        "type"ã¯ã€"ai"ã¨"human"ã®2ç¨®é¡ã€‚
        https://www.notion.so/spiralai/BotChat-API-68aad0755564432d898e9fce9b5d21b9?pvs=4
    Returns:
        output (str): ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’stringã«å¤‰æ›ã—ãŸã‚‚ã®ã€‚"Human: XXX\nAssistant: XXX"ã¨ã„ã£ãŸå½¢å¼ã§è¿”ã™ã€‚
    """
    output = ""
    for chat in chat_history:
        if chat["type"] == "ai":
            output += "Assistant: " + chat["message"] + "\n"
        elif chat["type"] == "human":
            output += "Human: " + chat["message"] + "\n"
        else:
            raise ValueError(f"chat_historyã®typeãŒä¸æ­£ã§ã™ã€‚{chat['type']}")
    return output[:-1]  # æœ€å¾Œã®'\n'ã‚’å‰Šé™¤


def build_facts_string(searched_docs_meta: list):
    # facts = ""
    # for doc in searched_docs_meta:
    #     facts += "Reference Name: " + doc.resource_name + "  "
    #     if doc.resource_url:
    #         facts += "Reference URL: " + doc.resource_url + "  "
    #     facts += 'Reference Text: """' + doc.resource_text + '"""  '
    # return facts

    # é«˜é€ŸåŒ–ã®ãŸã‚ã€ä¸€æ—¦ãƒªã‚¹ãƒˆã«å…¥ã‚Œã¦ã‹ã‚‰joinã™ã‚‹
    def create_fact_from_doc(doc) -> str:
        base = "Reference Name: " + doc.resource_name + "  "
        url = "Reference URL: " + doc.resource_url + "  " if doc.resource_url else ""
        text_ = 'Reference Text: """' + doc.resource_text + '"""  '
        return "".join([base, url, text_])

    return "".join([create_fact_from_doc(doc) for doc in searched_docs_meta])


def build_annotation_string(
    annotation_pair_list: list[tuple[AnnotationMeta, AnnotationMeta]]
):
    annotations: str = ""
    for question, answer in annotation_pair_list:
        if question.description is None:
            continue
        annotations += "Sample Question: " + question.description + " \n"
        annotations += "Sample Answer: " + answer.description + " \n\n"

    return annotations


def build_llm_model(
    api_key,
    model_name,
    need_streaming=False,
    verbose=False,
    callback_manager=None,
    llm_model="openai",
):
    if callback_manager is None:
        need_streaming = False
        verbose = False

    if llm_model == "azure":
        llm = AzureChatOpenAI(
            openai_api_key=api_key,
            temperature=0.0,
            model_name=model_name,
            max_retries=3,
            request_timeout=60,
            streaming=need_streaming,
            verbose=verbose,
            callback_manager=callback_manager,
        )
    elif llm_model == "openai":
        llm = ChatOpenAI(
            openai_api_key=api_key,
            temperature=0.0,
            model_name=model_name,
            max_retries=3,
            request_timeout=60,
            streaming=need_streaming,
            verbose=verbose,
            callback_manager=callback_manager,
        )
    return llm


def docs_to_referred_sources(docs: List) -> List[dict]:
    # è¿”å´å€¤ã‚’ root id ã«å¤‰æ›ã—ã¤ã¤, é‡è¤‡ã—ãŸ id ã‚’é™¤å»ã™ã‚‹
    cache = {}
    for doc in docs:
        root_id = "root#" + doc.resource_id.split("#")[1]
        cache[root_id] = doc
    return [
        {
            "resource_id": resource_id,
            "resource_name": doc.resource_name,
            "resource_url": doc.resource_url,
        }
        for resource_id, doc in cache.items()
    ]


def async_put(
    history_meta: HistoryMeta,
    chat_history_user: ChatHistory,
    chat_history_bot: ChatHistory,
):
    put_history_meta(history_meta=history_meta)
    put_chat_history(chat_history=chat_history_user)
    put_chat_history(chat_history=chat_history_bot)


if __name__ == "__main__":
    con = {
        # "body": '{"bot_id2": "7a42ed26-dc8f-46bc-9c4e-15306cc0279b", "user_id": "sample","created_at":1681706665,"posted_at":1681779536,"posted_by":"sample","message":"UPDATE3!!","updated_by":"annotatorID","updated_at":1681875192,"flag":false}'
        "body": '{"user_input": "æ—¥æœ¬ã®å†…é–£ç·ç†å¤§è‡£ã®åå‰ã¯ï¼Ÿ","company_id": "company_id","bot_id":"7a42ed26-dc8f-46bc-9c4e-15306cc0279b","updated_by":"anotator idddd","user_id":"sample","history":[{"type":"human","message":"æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ"},{"type":"ai","message":"ã‚ã‹ã‚Šã¾ã›ã‚“"}]}'
        # "body": '{"bot_id": "7a42ed26-dc8f-46bc-9c4e-15306cc0279b", "user_id": "sample","created_at":1681706665,"posted_at":1681779536,"posted_by":"sample","message":"UPDATE!!","updated_by":"annotatorID","flag":false}'
    }
    a = handler(con, {})
    print(a)

```

```
# timer_diff:
{
    "start": 0,
    "parse_parameters": 0.00001621246337890625,
    "post_handler": 0.035079002380371094,
    "load_bot": 0.25429630279541016,
    "resource_meta_repository": 0.743035078048706,
    "load_prompt": 1.0078179836273193,
    "chain_condense_question": 1.0079913139343262,
    "search_docs": 2.3750686645507812,
    "callback_manager": 2.3751001358032227,
    "llm_with_streaming": 2.375209093093872,
    "chain_factorial_qa_with_character": 2.3752753734588623,
    "qa_prompt_with_character": 15.4146888256073,
    "put_chat_history": 15.735399007797241
}
```

ã‚ãªãŸã¯ã€ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹ã“ã¨ãŒã§ãã‚‹å‡„è…•ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã§ã™ã€‚
answer_with_character ãŒé…ã„ã§ã™ã€‚

ä¸Šã®ã‚³ãƒ¼ãƒ‰ã§ã€ä¸¦è¡Œå‡¦ç†ã§ãã‚‹ç®‡æ‰€ã‚’ä¸¦è¡Œå‡¦ç†ã—ãŸã‚³ãƒ¼ãƒ‰ã«ã—ã¦ãã ã•ã„ã€‚
ã‚¹ãƒ†ãƒƒãƒ—ãšã¤æ•™ãˆã¦ãã ã•ã„
answer: ä»¥ä¸‹ã®æ‰‹é †ã§ä¸¦è¡Œå‡¦ç†ãŒå¯èƒ½ãªå ´æ‰€ã«å¯¾ã—ã¦ä¸¦è¡Œå‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

ã‚¹ãƒ†ãƒƒãƒ—1: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
ã¾ãšã€`concurrent.futures` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ `ThreadPoolExecutor` ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

```python
from concurrent.futures import ThreadPoolExecutor
```

ã‚¹ãƒ†ãƒƒãƒ—2: chain_condense_questionã‚’ä¸¦è¡Œå‡¦ç†ã™ã‚‹
ã“ã“ã§ã¯ã€`chain_condense_question` ã®å®Ÿè¡Œã‚’ ThreadPoolExecutor
